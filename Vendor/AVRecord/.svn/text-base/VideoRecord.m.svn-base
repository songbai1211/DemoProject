//
//  VideoRecord.m
//  DemoProject
//
//  Created by apple on 13-8-3.
//  Copyright (c) 2013年 apple. All rights reserved.
//

#import "VideoRecord.h"
#import "UIImage-Extensions.h"


@implementation VideoRecord


@synthesize captureSession  = _capSession;
@synthesize isRecording     = _isRecording;
@synthesize logoImage;
@synthesize logoRect;
@synthesize labelTime;
@synthesize saveVideoToImage;
@synthesize isFrontFace;
@synthesize timeLen;
@synthesize rotateSize;
@synthesize isRecordAudio;
@synthesize videoWidth;
@synthesize videoHeight;
@synthesize videoFrames;
@synthesize audioEncodeBitRate;
@synthesize videoEncodeBitRate;
@synthesize audioSampleRate;
@synthesize audioChannels;
@synthesize curFlashMode;
@synthesize videoOrientation;
@synthesize referenceOrientation;



/*
 1.读一个文件进行转换 ok
 2.镜头转换 ok
 3.叠图或文字 ok
 4.抓图保存为文件 ok
 5.补光灯，可以选择自动、打开、关闭三种状态；ok
 6.剪裁图片 ok
 7.录像中止或被中止的事件 ok
 8.检测是否有录制设备 ok
 9.输入输出参数化 ok
 10.支持双音频输入混音为一条单音轨
 11.内存释放 ok
 12.显示时间 ok
 */




#pragma mark -
#pragma mark Initialization
- (id)init {
	self = [super init];
	if (self) {
        
		/*We initialize some variables (they might be not initialized depending on what is commented or not)*/
		_imageView      = nil;
		_prevLayer      = nil;
		_customLayer    = nil;
        curFlashMode    = AVCaptureFlashModeAuto;
        audioSampleRate     = 44100/2;
        audioChannels       = 1;
        audioEncodeBitRate  = 24000;

        videoHeight         = 320;
        videoWidth          = 480;
        videoFrames         = 15;
        saveVideoToImage    = 5;
        isRecordAudio       = 1;
        videoEncodeBitRate  = 400*1000;
        referenceOrientation = UIDeviceOrientationPortrait;
        [self initRotateSize];
        [self initCapture];
	}
	return self;
}

-(void)initRotateSize {
    
    UIView *rotatedViewBox = [[UIView alloc] initWithFrame:CGRectMake(0, 0, videoWidth, videoHeight)];
    CGAffineTransform t = CGAffineTransformMakeRotation(-90.0/180*M_PI);
    rotatedViewBox.transform = t;
    rotateSize = rotatedViewBox.frame.size;
    NSLog(@"rotateSize=%f,%f",rotateSize.width, rotateSize.height);
}

- (void)initCapture {
    
    if([self cameraCount] <= 0)
        return;
    
    //创建session
    _capSession = [[AVCaptureSession alloc] init];
    
    //开始配置
    [_capSession beginConfiguration];
    
    //我们将选择的设备指定为中等质量。
    [_capSession setSessionPreset:AVCaptureSessionPresetMedium];
    
    
    //视频
    //找到一个合适的AVCaptureDevice
    AVCaptureDevice *device = [AVCaptureDevice defaultDeviceWithMediaType:AVMediaTypeVideo];
    
    //用device对象创建一个设备对象input，并将其添加到session
	_deviceVideo = [AVCaptureDeviceInput deviceInputWithDevice:device error:nil];
    AVCaptureDevicePosition position = [[_deviceVideo device] position];
    isFrontFace = (position == AVCaptureDevicePositionFront);
    NSLog(@"isFrontFace=%d", isFrontFace);
    [_capSession addInput:_deviceVideo];
    
    //创建一个VideoDataOutput对象，将其添加到session
	_captureVideo = [[AVCaptureVideoDataOutput alloc] init];
	/*While a frame is processes in -captureOutput:didOutputSampleBuffer:fromConnection: delegate methods no other frames are added in the queue.
	 If you don't want this behaviour set the property to NO */
	_captureVideo.alwaysDiscardsLateVideoFrames = YES;
	/*We specify a minimum duration for each frame (play with this settings to avoid having too many frames waiting
	 in the queue because it can cause memory issues). It is similar to the inverse of the maximum framerate.
	 In this example we set a min frame duration of 1/10 seconds so a maximum framerate of 10fps. We say that
	 we are not able to process more than 10 frames per second.*/
	_captureVideo.minFrameDuration = CMTimeMake(1, videoFrames);
	[_capSession addOutput:_captureVideo];
    
    //配置output对象
	dispatch_queue_t queueVideo;
	queueVideo = dispatch_queue_create("queueVideo", DISPATCH_QUEUE_SERIAL);
	[_captureVideo setSampleBufferDelegate:self queue:queueVideo];
	dispatch_release(queueVideo);
	
    //指定像素格式
	/*We create a serial queue to handle the processing of our frames*/
	// Set the video output to store frame in BGRA (It is supposed to be faster)
	NSString* key   = (NSString*)kCVPixelBufferPixelFormatTypeKey;
	NSNumber* value = [NSNumber numberWithUnsignedInt:kCVPixelFormatType_32BGRA];
	NSDictionary* videoSettings = [NSDictionary dictionaryWithObject:value forKey:key];
	[_captureVideo setVideoSettings:videoSettings];
    
    /*We use medium quality, ont the iPhone 4 this demo would be laging too much, the conversion in UIImage and CGImage demands too much ressources for a 720p resolution.*/
    
    //音频:
    if(isRecordAudio){
        dispatch_queue_t queueAudio;
        queueAudio = dispatch_queue_create("queueAudio", NULL);
        
        _deviceAudio = [AVCaptureDeviceInput
                        deviceInputWithDevice:[AVCaptureDevice defaultDeviceWithMediaType:AVMediaTypeAudio]
                        error:nil];
        _captureAudio =  [[AVCaptureAudioDataOutput alloc] init];
        
        //[_captureAudio setSampleBufferDelegate:self queue:dispatch_get_main_queue()];
        [_captureAudio setSampleBufferDelegate:self queue:queueAudio];
        [_capSession addInput:_deviceAudio];
        [_capSession addOutput:_captureAudio];
        dispatch_release(queueAudio);
    }
    
    //提交配置
    [_capSession commitConfiguration];
    
    int temp;
    for(int i=0;i<[[_captureVideo connections] count];i++){
        AVCaptureConnection* p = [[_captureVideo connections] objectAtIndex:i];
        NSLog(@"p=%d,%d" ,p.videoOrientation,p.supportsVideoOrientation);
        temp = p.videoOrientation;
        //NSLog(@"p=%f,%d",p.videoMinFrameDuration.value/p.videoMinFrameDuration.timescale,p.videoOrientation);
    }
    self.videoOrientation = temp;
    
    //videoConnection = [_captureVideo connectionWithMediaType:AVMediaTypeVideo];
    //self.videoOrientation = [videoConnection videoOrientation];
}

-(void)createPreview:(UIView*)parentView{
    
    /*	_customLayer = [CALayer layer];
     _customLayer.frame = parentView.bounds;
     _customLayer.transform = CATransform3DRotate(CATransform3DIdentity, M_PI/2.0f, 0, 0, 1);
     _customLayer.contentsGravity = kCAGravityResizeAspectFill;
     [parentView.layer addSublayer:_customLayer];
     
     _imageView = [[UIImageView alloc] init];
     _imageView.frame = parentView.bounds;
	 [parentView addSubview:_imageView];*/
    
    //创建预览层，并添加到parentView上
	_prevLayer = [[AVCaptureVideoPreviewLayer alloc] initWithSession: _capSession];
	_prevLayer.frame = parentView.bounds;
	_prevLayer.videoGravity = AVLayerVideoGravityResizeAspectFill;
    parentView.layer.masksToBounds = YES;
    //[parentView.layer addSublayer: _prevLayer];
    [parentView.layer insertSublayer:_prevLayer below:[[parentView.layer sublayers] objectAtIndex:0]];
    
    [_capSession startRunning];
}

#pragma mark -
#pragma mark AVCaptureSession delegate
- (void)captureOutput:(AVCaptureOutput *)captureOutput
didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer
	   fromConnection:(AVCaptureConnection *)connection
{
    /*We create an autorelease pool because as we are not in the main_queue our code is
	 not executed in the main thread. So we have to create an autorelease pool for the thread we are in*/
	
    
    // a very dense way to keep track of the time at which this frame
    // occurs relative to the output stream, but it's just an example!
    if(_isRecording){
        CMTime t = CMSampleBufferGetOutputPresentationTimeStamp(sampleBuffer);
        NSLog(@"%lld,%d",t.value, t.timescale);
        if(_startSessionTime.value == 0){
            //if( _writer.status ==  AVAssetWriterStatusUnknown && _startSessionTime.value == 0){
            _startSessionTime = CMSampleBufferGetPresentationTimeStamp(sampleBuffer);
            [_writer startWriting];
            [_writer startSessionAtSourceTime:_startSessionTime];
            NSLog(@"start=%lld, %d",t.value, t.timescale);
            return;
        }
        
        if( _writer.status <=  AVAssetWriterStatusWriting )
        {
            //视频数据
            if(captureOutput == _captureVideo)
                if(_videoInput.readyForMoreMediaData){
                    _writeVideoCount++;
                    [self changeSample:sampleBuffer];
                    if( _writer.status <=  AVAssetWriterStatusWriting )
                        [_videoInput appendSampleBuffer:sampleBuffer];
                    [self showRecordTime:sampleBuffer];
                    //static int64_t frameNumber = 0;
                    //[pixelBufferAdaptor appendPixelBuffer:imageBuffer withPresentationTime:CMTimeMake(frameNumber, 12)];
                    //frameNumber++;
                    //NSLog(@"video");
                }
            
            //音频数据
            if(captureOutput == _captureAudio)
                if(_audioInput.readyForMoreMediaData){
                    //CMSampleBufferSetOutputPresentationTimeStamp(sampleBuffer,CMTimeMakeWithSeconds(_lastTime,30));
                    [_audioInput appendSampleBuffer:sampleBuffer];
                    _writeAudioCount++;
                    NSLog(@"audio");
                }
        }
    }
}

-(void)showRecordTime:(CMSampleBufferRef)sampleBuffer {
    
    if(labelTime){
        CMTime t = CMSampleBufferGetOutputPresentationTimeStamp(sampleBuffer);
        t = CMTimeSubtract(t,_startSessionTime);
        int m = (t.value/t.timescale)/60;
        int n = (t.value/t.timescale)%60;
        timeLen = t.value/t.timescale;
        NSString* s = [NSString stringWithFormat:@"%.2d:%.2d",m,n];
        if( ![_lastShowTime isEqualToString:s] ){
            [labelTime performSelectorOnMainThread:@selector(setText:) withObject:s waitUntilDone:YES];
            _lastShowTime = s;
        }
    }
}

-(void)changeSample:(CMSampleBufferRef)sampleBuffer {
    
    
    @autoreleasepool {
	//NSAutoreleasePool * pool = [[NSAutoreleasePool alloc] init];
    /*Lock the image buffer*/
    //为媒体数据设置一个CMSampleBuffer的Core Video图像缓存对象
    CVImageBufferRef imageBuffer=NULL;
    imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);
    
    //锁定pixel buffer的基地址
    CVPixelBufferLockBaseAddress(imageBuffer,0);
    
    /*Get information about the image*/
    //得到pixel buffer的基地址
    uint8_t *baseAddress = (uint8_t *)CVPixelBufferGetBaseAddress(imageBuffer);
    
    //得到pixel buffer的行字节数以及宽和高
    size_t bytesPerRow = CVPixelBufferGetBytesPerRow(imageBuffer);
    size_t width = CVPixelBufferGetWidth(imageBuffer);
    size_t height = CVPixelBufferGetHeight(imageBuffer);
    
    /*Create a CGImageRef from the CVImageBufferRef*/
    //创建一个依赖于设备的RGB颜色空间
    CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB();
    
    //用抽样缓存的数据创建一个位图格式的图形上下文（graphics context）对象
    CGContextRef newContext = CGBitmapContextCreate(baseAddress, width, height, 8, bytesPerRow, colorSpace, kCGBitmapByteOrder32Little | kCGImageAlphaPremultipliedFirst);
    
    //根据这个位图context中的像素数据创建一个Quartz image对象
    CGImageRef newImage = CGBitmapContextCreateImage(newContext);
    
    //	UIGraphicsBeginImageContext(CGSizeMake(videoHeight, videoWidth));
    //	CGContextTranslateCTM(newContext, rotateSize.width/2, rotateSize.height/2);
    //	CGContextRotateCTM(newContext, -90.0/180*M_PI);
    //	CGContextScaleCTM(newContext, 1.0, -1.0);
    //  CGContextDrawImage(newContext, CGRectMake(-videoWidth/2, -videoHeight/2, videoWidth, videoHeight), newImage);
    
    //保存
    if(logoImage)
        CGContextDrawImage(newContext,logoRect,logoImage.CGImage);
	if(saveVideoToImage)
        [self saveToImage:sampleBuffer newImage:newImage];
    
	//[_customLayer performSelectorOnMainThread:@selector(setContents:) withObject: (id) newImage waitUntilDone:YES];
	//[_imageView performSelectorOnMainThread:@selector(setImage:) withObject:image waitUntilDone:YES];
    //_imageView.image = [UIImage imageWithCGImage:newImage];
    
	
    //释放context和颜色空间 释放Quartz image对象 解锁pixel buffer
    CGContextRelease(newContext);
    CGColorSpaceRelease(colorSpace);
    CGImageRelease(newImage);
	CVPixelBufferUnlockBaseAddress(imageBuffer,0);
	
	//[pool drain];
    }
}

- (CGRect) getBoundingRectAfterRotation: (CGRect) rectangle byAngle: (CGFloat) angleOfRotation {
    // Calculate the width and height of the bounding rectangle using basic trig
    CGFloat newWidth = rectangle.size.width * fabs(cosf(angleOfRotation)) + rectangle.size.height * fabs(sinf(angleOfRotation));
    CGFloat newHeight = rectangle.size.height * fabs(cosf(angleOfRotation)) + rectangle.size.width * fabs(sinf(angleOfRotation));
    
    // Calculate the position of the origin
    CGFloat newX = rectangle.origin.x + ((rectangle.size.width - newWidth) / 2);
    CGFloat newY = rectangle.origin.y + ((rectangle.size.height - newHeight) / 2);
    
    // Return the rectangle
    return CGRectMake(newX, newY, newWidth, newHeight);
}

- (NSString *)dataFilePath {
    NSString* s = [NSString stringWithFormat:@"%@/Library/Caches/",NSHomeDirectory()];
    //NSLog(@"%@",s);
    return s;
}

-(void)saveToImage:(CMSampleBufferRef)sampleBuffer newImage:(CGImageRef)newImage{
    
    CMTime n = CMSampleBufferGetPresentationTimeStamp(sampleBuffer);
    n = CMTimeSubtract(n,_startSessionTime);
    int m = n.value / n.timescale;
    if(m % saveVideoToImage == 0){
        
        //保存图片文件到cache目录
        NSString* s = [NSString stringWithFormat:@"%@capture_video_%d.jpg",[self dataFilePath],m];
        if(![s isEqualToString:_lastSaveFile]){
            UIImage *image= [UIImage imageWithCGImage:newImage scale:1.0 orientation:UIImageOrientationRight];
            image = [image imageAtRect:CGRectMake(80, 80, 320, 320)];
            image = [image imageRotatedByDegrees:90];
            NSData* data = UIImageJPEGRepresentation(image,0.8f);
            NSLog(@"saveToImage:%@",s);
            [data writeToFile:s atomically:YES];
            image = nil;
            data  = nil;
            _lastSaveFile = s;
        }
    }
}

#pragma mark -
#pragma mark Memory management

- (void)viewDidUnload {
	_imageView = nil;
	_customLayer = nil;
	_prevLayer = nil;
}



- (BOOL) createWriter
{
    //    NSString *file = [self file];
    NSString *file = [[self dataFilePath]stringByAppendingPathComponent:@"1.mp4"];
    
    if ([[NSFileManager defaultManager] fileExistsAtPath:file]) [[NSFileManager defaultManager] removeItemAtPath:file error:NULL];
    
    NSError *error = nil;
    _writer = [[AVAssetWriter alloc] initWithURL:[NSURL fileURLWithPath:file] fileType:AVFileTypeMPEG4 error:&error];
    
    if (error)
    {
        
        NSLog(@"%@", [error description]);
        _writer = nil;
        return NO;
    }
    
    NSDictionary *settings;
    if(isRecordAudio){
        AudioChannelLayout acl;
        bzero( &acl, sizeof(acl));
        if(audioChannels>=2)
            acl.mChannelLayoutTag = kAudioChannelLayoutTag_Stereo;
        else
            acl.mChannelLayoutTag = kAudioChannelLayoutTag_Mono;
        
        settings = [NSDictionary dictionaryWithObjectsAndKeys:
                    [NSNumber numberWithInt:kAudioFormatMPEG4AAC],  AVFormatIDKey,
                    [NSNumber numberWithFloat:audioSampleRate],     AVSampleRateKey,
                    [NSNumber numberWithInt:audioChannels],         AVNumberOfChannelsKey,
                    [NSNumber numberWithInt:audioEncodeBitRate],    AVEncoderBitRateKey,
                    [NSData dataWithBytes:&acl length:sizeof(acl)], AVChannelLayoutKey,
                    nil ];
        
        _audioInput = [[AVAssetWriterInput alloc] initWithMediaType:AVMediaTypeAudio outputSettings:settings];
        _audioInput.expectsMediaDataInRealTime = YES;
        [_writer addInput:_audioInput];
    }
    
    NSDictionary *codecSettings = [NSDictionary dictionaryWithObjectsAndKeys:
                                   [NSNumber numberWithInt:videoEncodeBitRate], AVVideoAverageBitRateKey,
                                   [NSNumber numberWithInt:videoFrames],        AVVideoMaxKeyFrameIntervalKey,
                                   AVVideoProfileLevelH264Main31,               AVVideoProfileLevelKey,
                                   nil];
    
    settings = [NSDictionary dictionaryWithObjectsAndKeys:
                AVVideoCodecH264, AVVideoCodecKey,
                [NSNumber numberWithInt:videoWidth], AVVideoWidthKey,
                [NSNumber numberWithInt:videoHeight], AVVideoHeightKey,
                codecSettings,AVVideoCompressionPropertiesKey,
                nil];
    
    _videoInput = [[AVAssetWriterInput alloc] initWithMediaType:AVMediaTypeVideo outputSettings:settings];
    _videoInput.expectsMediaDataInRealTime = YES;
    [_writer addInput:_videoInput];
    
    _videoInput.transform = [self transformFromCurrentVideoOrientationToOrientation:self.referenceOrientation];
    
    //pixelBufferAdaptor = [[AVAssetWriterInputPixelBufferAdaptor alloc] initWithAssetWriterInput:_videoInput
    //sourcePixelBufferAttributes:[NSDictionary dictionaryWithObjectsAndKeys:[NSNumber numberWithInt:kCVPixelFormatType_32BGRA],    kCVPixelBufferPixelFormatTypeKey,
    //nil]];
    return YES;
}

- (void) deleteWriter
{

    _videoInput = nil;
    _audioInput = nil;
    _writer     = nil;
}


- (BOOL)canRecordBuffer:(CMSampleBufferRef)sampleBuffer
{
    return YES;
}

- (void) RecordingAudioWithBuffer:(CMSampleBufferRef)sampleBuffer
{
    if (![self canRecordBuffer:sampleBuffer])
        return;
    
    if ([_audioInput isReadyForMoreMediaData])
        [_audioInput appendSampleBuffer:sampleBuffer];
}

- (void) RecordingVideoWithBuffer:(CMSampleBufferRef)sampleBuffer
{
    if (![self canRecordBuffer:sampleBuffer])
        return;
    
    if ([_videoInput isReadyForMoreMediaData])
        [_videoInput appendSampleBuffer:sampleBuffer];
}


-(void) startRecord
{
    if( !_isRecording )
    {
        _startSessionTime.value = 0;
        if( _writer == nil){
            if( ![self createWriter] ) {
                NSLog(@"Setup Writer Failed") ;
                return;
            }
        }
        
        if(!_capSession.running)
            [_capSession startRunning];
        
        
        _isRecording = YES;
        NSLog(@"start video recording...");
    }
}

-(void) stopRecord
{
    if( _isRecording )
    {
        _isRecording = NO;
        [_capSession stopRunning] ;
        [_videoInput markAsFinished];
        [_audioInput markAsFinished];
        if(![_writer finishWriting]) {
            NSLog(@"finishWriting returned NO") ;
        }
    
        _videoInput = nil;
        _audioInput = nil;
        _writer     = nil;
        _startSessionTime.value = 0;
        NSLog(@"video recording stopped:%d frames,%d audios",_writeVideoCount,_writeAudioCount);
    }
}

-(void)setting{
    AVCaptureConnection *videoConnection = NULL;
    
    [_capSession beginConfiguration];
    
    for ( AVCaptureConnection *connection in [_captureVideo connections] )
    {
        for ( AVCaptureInputPort *port in [connection inputPorts] )
        {
            if ( [[port mediaType] isEqual:AVMediaTypeVideo] )
            {
                videoConnection = connection;
            }
        }
    }
    if([videoConnection isVideoOrientationSupported]) // **Here it is, its always false**
    {
        [videoConnection setVideoOrientation:AVCaptureVideoOrientationPortrait];
    }
    [_capSession commitConfiguration];
}

-(void)clearQueue{
//    dispatch_queue_t queue = dispatch_queue_create("queueVideo", NULL);
//    dispatch_set_context(queue, (__bridge void *)(self));
//    dispatch_set_finalizer_f(queue, _captureVideo);
//    [_captureVideo setSampleBufferDelegate: self queue: queue];
//    dispatch_release(queue);
}

// Toggle between the front and back camera, if both are present.
- (BOOL) toggleCamera
{
    BOOL success = NO;
    
    if ([self cameraCount] > 1) {
        NSError *error;
        AVCaptureDeviceInput *newVideoInput;
        AVCaptureDevicePosition position = [[_deviceVideo device] position];
        
        if (position == AVCaptureDevicePositionBack){
            newVideoInput = [[AVCaptureDeviceInput alloc] initWithDevice:[self frontFacingCamera] error:&error];
            isFrontFace = YES;
        }
        else if (position == AVCaptureDevicePositionFront){
            newVideoInput = [[AVCaptureDeviceInput alloc] initWithDevice:[self backFacingCamera] error:&error];
            isFrontFace = NO;
        }
        else
            goto bail;
        
        if (newVideoInput != nil) {
            [_capSession beginConfiguration];
            [_capSession removeInput:_deviceVideo];
            if ([_capSession canAddInput:newVideoInput]) {
                [_capSession addInput:newVideoInput];
            } else {
                [_capSession addInput:_deviceVideo];
            }
            [_capSession commitConfiguration];
            success = YES;
            _deviceVideo = newVideoInput;
        } else if (error) {
//            if ([[self delegate] respondsToSelector:@selector(captureManager:didFailWithError:)]) {
//                [[self delegate] captureManager:self didFailWithError:error];
//            }
        }
    }
    
bail:
    return success;
}


#pragma mark Device Counts

//摄像头个数
- (NSUInteger) cameraCount
{
    return [[AVCaptureDevice devicesWithMediaType:AVMediaTypeVideo] count];
}

//麦克风个数
- (NSUInteger) micCount
{
    return [[AVCaptureDevice devicesWithMediaType:AVMediaTypeAudio] count];
}

// Find a camera with the specificed AVCaptureDevicePosition, returning nil if one is not found
// 查找指定position的AVCaptureDevice
- (AVCaptureDevice *) cameraWithPosition:(AVCaptureDevicePosition) position
{
    NSArray *devices = [AVCaptureDevice devicesWithMediaType:AVMediaTypeVideo];
    for (AVCaptureDevice *device in devices) {
        if ([device position] == position) {
            return device;
        }
    }
    return nil;
}

// Find a front facing camera, returning nil if one is not found
//前置摄像头
- (AVCaptureDevice *) frontFacingCamera
{
    return [self cameraWithPosition:AVCaptureDevicePositionFront];
}

// Find a back facing camera, returning nil if one is not found
//后置摄像头
- (AVCaptureDevice *) backFacingCamera
{
    return [self cameraWithPosition:AVCaptureDevicePositionBack];
}

// Find and return an audio device, returning nil if one is not found
//音频设备
- (AVCaptureDevice *) audioDevice
{
    NSArray *devices = [AVCaptureDevice devicesWithMediaType:AVMediaTypeAudio];
    if ([devices count] > 0) {
        return [devices objectAtIndex:0];
    }
    return nil;
}

//捕获通知事件
-(void)createNotify{
    NSNotificationCenter *notify = [NSNotificationCenter defaultCenter];
    [notify addObserver: self selector: @selector(onVideoError:) name: AVCaptureSessionRuntimeErrorNotification object: _capSession];
    [notify addObserver: self selector: @selector(onVideoInterrupted:) name: AVCaptureSessionWasInterruptedNotification object: _capSession];
    //[notify addObserver: self selector: @selector(onVideoEnded:) name: AVCaptureSessionInterruptionEndedNotification object: _capSession];
    //[notify addObserver: self selector: @selector(onVideoDidStopRunning:) name: AVCaptureSessionDidStopRunningNotification object: _capSession];
    //[notify addObserver: self selector: @selector(onVideoStart:) name: AVCaptureSessionDidStartRunningNotification object: _capSession];
    notify = nil;
}

//错误
-(void)onVideoError:(AVCaptureSession*)cap{
    [self stopRecord];
}

//中断
-(void)onVideoInterrupted:(AVCaptureSession*)cap{
    [self stopRecord];
}

//设置闪光灯模式
-(void)setFlashMode:(AVCaptureFlashMode)n{
    AVCaptureDevice* device = [AVCaptureDevice defaultDeviceWithMediaType:AVMediaTypeVideo];
    if(device.hasFlash){
        [device lockForConfiguration:nil];
        curFlashMode = n;
        device.torchMode = n;       //手电筒
        device.flashMode = n;       //闪光灯
        [device unlockForConfiguration];
    }
}


//视频转向
- (CGFloat)angleOffsetFromPortraitOrientationToOrientation:(AVCaptureVideoOrientation)orientation
{
	CGFloat angle = 0.0;
	
	switch (orientation) {
		case AVCaptureVideoOrientationPortrait:
			angle = 0.0;
			break;
		case AVCaptureVideoOrientationPortraitUpsideDown:
			angle = M_PI;
			break;
		case AVCaptureVideoOrientationLandscapeRight:
			angle = -M_PI_2;
			break;
		case AVCaptureVideoOrientationLandscapeLeft:
			angle = M_PI_2;
			break;
		default:
			break;
	}
    
	return angle;
}

- (CGAffineTransform)transformFromCurrentVideoOrientationToOrientation:(AVCaptureVideoOrientation)orientation
{
	CGAffineTransform transform = CGAffineTransformIdentity;
    
	// Calculate offsets from an arbitrary reference orientation (portrait)
	CGFloat orientationAngleOffset      = [self angleOffsetFromPortraitOrientationToOrientation:orientation];
	CGFloat videoOrientationAngleOffset = [self angleOffsetFromPortraitOrientationToOrientation:self.videoOrientation];
	
	// Find the difference in angle between the passed in orientation and the current video orientation
	CGFloat angleOffset = orientationAngleOffset - videoOrientationAngleOffset;
	transform = CGAffineTransformMakeRotation(angleOffset);
	
	return transform;
}

@end
